# Private AI Stack

A self-hosted private AI system (ChatGPT-style) running on Proxmox with Docker, GPU acceleration, and secure external access via Cloudflare Tunnel. Supports chat, vision, image generation, web search, and TTS/STT.

## âš¡ Quick Navigation
- **[Architecture](./docs/architecture.md)** â€” Hardware, network flow, threat boundaries
- **[Installation](./docs/installation.md)** â€” Host/VM prerequisites and setup
- **[Configuration](./docs/configuration.md)** â€” Service wiring, storage (NFS/ZFS), tuning
- **[Usage Guide](./docs/usage.md)** â€” Operatorâ€™s manual (daily workflows)

## Repository Layout
- `docker-compose.yml` â€“ Full stack definition
- `docs/`
  - `architecture.md` â€“ Diagrams and system design
  - `installation.md` â€“ Setup procedure
  - `configuration.md` â€“ Env vars, persistence, NFS/ZFS, GPU notes
  - `usage.md` â€“ How to use Open WebUI + ComfyUI
- `searxng/` â€“ SearXNG config (`settings.yml`, etc.)
- `tts/` â€“ OpenedAI-Speech voice models + mappings

## âœ¨ Features
- Multiple chat models (Llama 3.1, Qwen 7B, Gemma 27B)
- Image generation via ComfyUI backend
- Vision support (use the Qwen Vision model when analyzing images)
- Web search via SearXNG (toggle per message)
- Text-to-Speech via OpenedAI-Speech (OpenAI-compatible API)
- Speech-to-Text via Whisper/faster-whisper
- Persistent storage on NAS via NFS (`/mnt/ai`)
- Secure external access via Cloudflare Tunnel + MFA

## ğŸ§± Architecture (high-level)

```text
User â†’ Cloudflare Tunnel (+MFA) â†’ Open WebUI
                               â”œâ”€ Ollama (LLMs, on VM host)
                               â”œâ”€ SearXNG (web search)
                               â”œâ”€ OpenedAI-Speech (TTS)
                               â”œâ”€ Whisper (STT)
                               â””â”€ ComfyUI (image generation, GPU)
Storage: VM mounts NAS dataset via NFS at /mnt/ai (persistent volumes)
```
Full details: [docs/architecture.md](https://github.com/nicolasnkGH/ai-stack/blob/main/docs/architecture.md)

## âš™ï¸ Performance Notes (rule of thumb)

* Chat is mostly GPU-bound (CPU is usually low)

* Image generation is GPU-bound

* Typical VM sizing: 6â€“8 vCPU, 32â€“64GB RAM, GPU passthrough

---
Maintained by Nicolas Teixeira.

  
