# Private AI Stack

A self-hosted, private AI system (ChatGPT-style) running on Proxmox with Docker, GPU acceleration, and secure external access via Cloudflare Tunnel. Supports chat, vision, image generation, web search, and TTS/STT.

## âš¡ Quick Navigation

-   **[Architecture](./docs/architecture.md)** â€” Hardware, network flow, security boundaries
-   **[Installation](./docs/installation.md)** â€” Host/VM prerequisites and setup
-   **[Configuration](./docs/configuration.md)** â€” Service wiring, storage (NFS/ZFS), tuning
-   **[Usage Guide](./docs/usage.md)** â€” Operator's manual (daily workflows)

## Repository Layout

```text
.
â”œâ”€â”€ docker-compose.yml     # Main stack orchestration (GPU + Volumes)
â”œâ”€â”€ .env.example           # Template for secrets (Google Drive, URLs)
â”œâ”€â”€ .gitignore             # Prevents secrets and local databases from being committed
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ backup-db.sh       # Nightly database backup to NFS
â”œâ”€â”€ docs/                  # In-depth technical documentation
â”‚   â”œâ”€â”€ architecture.md    # Network flow & security boundaries
â”‚   â”œâ”€â”€ installation.md    # Host/VM setup & GPU passthrough
â”‚   â”œâ”€â”€ configuration.md   # Hybrid storage & VRAM tuning
â”‚   â””â”€â”€ usage.md           # Operator's manual for UI workflows
â”œâ”€â”€ searxng/               # Custom meta-search configuration
â”‚   â””â”€â”€ settings.yml       # Engine filters & privacy settings
â””â”€â”€ tts/                   # Speech synthesis assets
    â”œâ”€â”€ voices/            # Local voice model files
    â””â”€â”€ config/            # Audio mapping & endpoint settings
```

## âœ¨ System Features & Capabilities

Engineered for maximum throughput and local data sovereignty.

### ğŸ§  Intelligence & Next-Gen Models

Powered by **Ollama** & **RTX 3090 Ti**.

*   **LLM Support:** Llama 3.1, Qwen 2.5 Coder, Gemma 3 (12B/27B) â€“ handles vision and text.
*   **Web Search:** Privacy-first RAG via self-hosted SearXNG.
*   **Cloud Integration (Optional):** Secure Google Drive access.

### âš¡ Performance & Hybrid Infrastructure

Dual-storage eliminates I/O bottlenecks.

*   **Storage:** Critical data (LLM models, databases, Redis) on local NVMe/SSD (`/opt/ai`); bulk assets on NFS (`/mnt/ai`).
*   **Image Generation:** ComfyUI for node-based workflows.
*   **Audio:** OpenAI-compatible TTS & STT (local).
*   **VRAM Management:** Dynamic `keep_alive` ensures **24GB VRAM** availability for ComfyUI workflows.

### ğŸ›¡ï¸ Security & Sovereignty

Enterprise-grade protection for a 100% private AI experience.

*   **Remote Access:** Zero-trust via Cloudflare Tunnel + MFA.
*   **Data Sovereignty:** All processing local in Columbus, Ohio. No data leaves the host.

---

## ğŸ§± Architecture (high-level)

```text
User â†’ Cloudflare Tunnel (+MFA) â†’ Open WebUI
                               â”œâ”€ Ollama (Dockerized LLM Engine)
                               â”œâ”€ SearXNG (web search)
                               â”œâ”€ OpenedAI-Speech (TTS)
                               â”œâ”€ Whisper (STT)
                               â””â”€ ComfyUI (image generation, GPU)
                               â””â”€ [Optional] Google Drive (External Docs)

Storage: VM mounts NAS dataset via NFS at /mnt/ai (persistent volumes)
```
Full details: [docs/architecture.md](https://github.com/nicolasnkGH/ai-stack/blob/main/docs/architecture.md)

## âš™ï¸ Performance Notes (rule of thumb)

* Chat is mostly GPU-bound (CPU is usually low)

* Image generation is GPU-bound

* Typical VM sizing: 6â€“8 vCPU, 32â€“64GB RAM, GPU passthrough

* **Hybrid Storage**: Performance-critical data (LLM models, Databases, Redis) resides on **local SSD (`/opt/ai`)** to eliminate 1Gbps NFS latency and GPU starvation.
* **Bulk Storage**: Large assets (Image outputs, RAG uploads) remain on **NFS (`/mnt/ai`)** to leverage NAS capacity without impacting UI snappiness.
* **VRAM Management**: Main models use a 5-10 minute `keep_alive` to ensure the **24GB VRAM** is freed for ComfyUI workflows.

---

## ğŸš€ Quick Start
1. **Prepare Host Directories**:
   ```bash
   sudo mkdir -p /opt/ai/{ollama,comfyui/models,open-webui,redis}
   sudo chown -R 1000:1000 /opt/ai
   ```
2. **Configure Secrets - Optional for Google Drive Integration**:
   ```bash
   # Edit .env with your Google Drive API keys
   ```
3. **Launch:**
   ```bash
   docker compose up -d
   ```


### â˜ï¸ Optional: Google Drive Integration
To enable Google Drive support for document RAG, you must follow the [official Open WebUI Google Drive Guide](https://docs.openwebui.com/reference/env-configuration#google):
1. Create a project in the Google Cloud Console.
2. Enable the Google Drive API.
3. Configure the OAuth consent screen and create credentials.
4. Uncomment the Google Drive section in `docker-compose.yml` and add your keys to `.env`.

---

## ğŸ“¸ Proof of Life

### ğŸ–¥ï¸ System & Hardware
| Host Specifications | GPU Status (`nvidia-smi`) |
| :--- | :--- |
| ![VM Info](./screenshots/vm-configuration.png) | ![GPU Usage](./screenshots/nvidia-smi.png) |

<details>
<summary><b>ğŸ” View Docker Stack Health</b></summary>

![Docker Stack](./screenshots/docker-compose-ps.png)
</details>

---

### ğŸ’¬ Open WebUI Interface
*A unified hub for LLMs, RAG, and multimodal workflows.*

| Model Selection & Fast-Switching | Advanced RAG + Web Search |
| :--- | :--- |
| ![Chat](./screenshots/openwebui-chat.png) | ![Web Search](./screenshots/openwebui-websearch.png) |

| Vision & Multimodal Analysis | Image Generation (DALL-E 3 Style) |
| :--- | :--- |
| ![Vision](./screenshots/openwebui-vision.png) | ![Models](./screenshots/openwebui-models-generating-images.png) |

---

### ğŸ¨ Backend Services
<details>
<summary><b>ğŸ› ï¸ View API & ComfyUI Workflows</b></summary>

#### OpenAI-Compatible TTS API
![OpenAI-compatible endpoint](./screenshots/openai-endpoint.png)

#### ComfyUI Node-Based Workflow
![ComfyUI Workflow](./screenshots/comfyui-output.png)
</details>

---

## ğŸ’¾ Maintenance: Automated Backups
Since performance-critical data lives on the local VM disk, a safety-net script is provided to sync the database back to the NFS nightly.

1.  **Make script executable**:
    ```bash
    chmod +x ./scripts/backup-db.sh
    ```
    
3.  **Add to Crontab**:
    ```bash
    # Runs daily at 3:00 AM
    (crontab -l 2>/dev/null; echo "0 3 * * * /home/ubuntu/ai-stack/scripts/backup-db.sh") | crontab -
    ```

---
*Maintained by Nicolas Teixeira.*
  
